<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Michael Yuan | Ë¢ÅÊâøÂçö</title>
  <meta name="author" content="Michael Yuan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/my/icon.jpg">
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?a4292ca8f2fe5fd7dc6dfd78cc894aab";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:68%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Michael (Chengbo) Yuan &nbsp | &nbsp Ë¢ÅÊâøÂçö</name>
                </p>
                <p class="larger-text">
                  ü¶äü¶ÅüêÆüêπüê∞üê∏üê∂üêªüê®üêØ

                  </br></br>  

                  I am a Master student in <a class="larger-text" href="https://iiis.tsinghua.edu.cn/en/" target="_blank">Institute for Interdisciplinary Information Science (IIIS)</a> at <a class="larger-text" href="https://www.tsinghua.edu.cn/en/" target="_blank">Tsinghua University</a>, advised by <a class="larger-text" href="https://people.iiis.tsinghua.edu.cn/~gaoyang/yang-gao.weebly.com/index.html" target="_blank">Prof. Yang Gao</a>ü•≥. 
                  Currently, my research interests mainly focus on <b>Embodied AI (finding a way to achieve scalable robot learning)</b>, with the assistance of <b>3D computer vision (3DV)</b>.
                  
                  </br></br>
                  Perviously, I have interned at <a class="larger-text" href="https://www.moonshot.cn/">MoonshotAI</a> and <a class="larger-text" href="https://www.mindspore.cn/">MindSpore of Huawei</a>. I am also interested in creating AI products or startups. 
                  I received my bachelor degree from the <a class="larger-text" href="http://hyxt.whu.edu.cn/" target="_blank">HongYi Honor class</a> at the <a class="larger-text" href="https://cs.whu.edu.cn/" target="_blank">School of CS</a>, <a class="larger-text" href="https://en.whu.edu.cn/" target="_blank">Wuhan Universityüòò</a>,
                  under the supervision of <a class="larger-text" href="https://scholar.google.com/citations?user=zb1oVGIAAAAJ&hl=zh-CN&oi=ao" target="_blank">Prof. Yong Luo</a>. I am also minoring in Law at <a class="larger-text" href="https://law.whu.edu.cn/English_new/Home.htm" target="_blank">School of Law</a> and have a broad and extensive interest in philosophy and finance.

                  </br></br>
                  I welcome <b style="font-size: 19px;">exploration, creation and excitement!</b> ü§óü§©ü§™
                </p>
                <p style="text-align:center">
                  <a class="larger-text" href="https://scholar.google.com/citations?user=ehrpcBwAAAAJ&hl=zh-CN">Google Scholar</a>&nbsp/&nbsp
                  <a class="larger-text" href="https://github.com/michaelyuancb">Github</a>&nbsp/&nbsp
                  <a class="larger-text" href="https://x.com/michaelyuancb">X(Twitter)</a>&nbsp/&nbsp
                  <a class="larger-text" href="images/my/wechat_blog.jpg">WeBlog</a>&nbsp/&nbsp
                  <a class="larger-text" href="https://www.zhihu.com/people/guessycb">Zhihu</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/my/photo.jpg"><img style="width:95%;max-width:95%" alt="profile photo" src="images/my/photo.jpg" class="hoverZoomLink"></a>
                <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fmichaelyuancb.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
                <br>
                contact me:
                <a class="larger-text" href="mailto:ycb24@mails.tsinghua.edu.cn">ycb24@mails.tsinghua.edu.cn</a>
                <a class="larger-text" href="mailto:michaelyuancb@163.com">michaelyuancb@163.com</a> 
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <heading>Recent</heading>
            
            <div class="slider recent-work">

              <div class="recent-block">
                <a href="https://general-flow.github.io/">
                  <video muted autoplay loop>
                    <source src="images/research/roboengine.mp4" type="video/mp4">
                  </video>
                </a>
                <span style="width: 390px; text-align: center;">
                  We release RoboEngine, the first plug-and-play and generalizable robot segmentation and augmentation models ! 
                </span>
              </div>

              <div class="recent-block">
                <img src="images/recent/scalable-robot-learning.png">
                <span style="width: 370px; text-align: center;">
                  A pathway I envision for scalable robot intelligence, which forms the central theme of my research.
                </span>
              </div>

              <div class="recent-block">
                <a href="https://general-flow.github.io/">
                  <video muted autoplay loop>
                    <source src="images/research/general_flow.mp4" type="video/mp4">
                  </video>
                </a>
                <span style="width: 390px; text-align: center;">
                  Check out General Flow as a possible representation for scalable robot learning !  
                </span>
              </div>

            </div>
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Research</heading>
          <p class="larger-text">
          My research interests primarily focus on <b>Embodied AI</b> and <b>3D Computer Vision</b>, particularly in utilizing vision-based methods to develop intelligent robotic systems <b>with scalable ways</b>. 
          My ultimate aspiration is to create general-propose robotsüòé that can truly understand and interact with our physical world. 
          <br>
          (representative papers are <span style="background-color: lightyellow;">highlighted</span>)
          </p>

          <tr style="background-color: lightyellow;">
            <td style="padding:10px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two" id='general_flow_video'>
                  <video  width=140% muted autoplay loop>
                  <source src="images/research/roboengine.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle">
              <a href="https://general-flow.github.io/">
                <papertitle>RoboEngine: Plug-and-Play Robot Data Augmentation with Semantic Robot Segmentation and Background Generation</papertitle>
              </a>
              <br>
              <a class="author-color"><strong>Chengbo Yuan*</strong></a>, 
              <a href="https://x.com/nonlinearjunkie" class="author-color">Suraj Joshi*</a>,
              <a href="https://zst1406217.github.io//" class="author-color">Shaoting Zhu*</a>,
              <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=en" class="author-color">Hang Su</a>,
              <a href="https://hangzhaomit.github.io/" class="author-color">Hang Zhao</a>,
              <a href="https://yang-gao.weebly.com/" class="author-color">Yang Gao</a>
              <br>
              <a href="https://roboengine.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2503.18738">arXiv</a>
              /
              <a href="https://github.com/michaelyuancb/roboengine">code</a>
              /
              <a href="https://huggingface.co/datasets/michaelyuanqwq/roboseg">dataset</a>
              <br>
              <strong>In Submission, 2025</strong>
              <p> 
                RoboEngine is the first plug-and-play visual robot data augmentation toolkit. Users can effortlessly generate physics-aware robot scenes with few lines code. 
                This enable training only in one scenes and visual generalizing to almost arbitrary scenes.
              </p>
              
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:26%;vertical-align:middle">
              <div class="one">
                <div class="two" id='general_flow_video'>
                  <video  width=140% muted autoplay loop>
                  <source src="images/research/egomono4d.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle">
              <a href="https://general-flow.github.io/">
                <papertitle>Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos</papertitle>
              </a>
              <br>
              <a class="author-color"><strong>Chengbo Yuan</strong></a>, 
              <a href="https://jc043.github.io/" class="author-color">Geng Chen</a>,
              <a href="https://ericyi.github.io/" class="author-color">Li Yi</a>,
              <a href="https://yang-gao.weebly.com/" class="author-color">Yang Gao</a>
              <br>
              <a href="https://egomono4d.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2411.09145">arXiv</a>
              /
              <a href="https://github.com/michaelyuancb/egomono4d">code</a>
              /
              <a href="https://x.com/michaelyuancb/status/1906287209369485539"> thread</a>
              <br>
              <strong>In Submission, 2025</strong>
              <p> 
                We train EgoMono4D, a fast, dense and generalizable 4D reconstruction model for egocentric videos with label-free self-supervised methods. 
                The training is conducted sole on unlabeled videos, potentially to apply to more label-scarce fields in the future. 
              </p>
              
            </td>
          </tr>

          <tr style="background-color: lightyellow;">
            <td style="padding:10px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='general_flow_video'>
                  <video  width=140% muted autoplay loop>
                  <source src="images/research/general_flow.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle">
              <a href="https://general-flow.github.io/">
                <papertitle>General Flow as Foundation Affordance for Scalable Manipulation Learning</papertitle>
              </a>
              <br>
              <a class="author-color"><strong>Chengbo Yuan</strong></a>, 
              <a href="https://alvinwen428.github.io/" class="author-color">Chuan Wen</a>,
              <a href="https://tongzhangthu.github.io/" class="author-color">Tong Zhang</a>,
              <a href="https://yang-gao.weebly.com/" class="author-color">Yang Gao</a>
              <br>
              <a href="https://general-flow.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2401.11439">arXiv</a>
              /
              <a href="https://github.com/michaelyuancb/general_flow">code</a>
              /
              <a href="https://openreview.net/forum?id=nmEt0ci8hi">openreview</a>
              /
              <a href="https://x.com/ChuanWen15/status/1831542292265812238">thread</a>
              <br>
              <strong>Conference on Robot Learning (CoRL), 2024</strong>
              <p> 
                We build a 3D flow prediction model directly from large-scale RGBD human video datasets.
                Based on this model, we achieve stable zero-shot human-to-robot skill transfer in the real world.
              </p>
              
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mmff'>
                  <img src="images/research/mmff.png" style="width:140%;">
                </div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle">
              <papertitle class="blue-color">Depression Diagnosis and Analysis via Multimodal Multi-order Factor Fusion</papertitle>
              <br>
              <a class="author-color"><strong>Chengbo Yuan</strong></a>, 
              <a class="author-color">Yongqian Li</a>,
              <a class="author-color">Qiancheng Yang</a>,
              <a class="author-color">Qianhui Xu</a>,
              <a href="https://scholar.google.com/citations?user=zb1oVGIAAAAJ&hl=zh-CN&oi=ao" class="author-color">Yong Luo</a>
              <br>
              <a href="https://arxiv.org/abs/2301.00254">arXiv</a>
              <br>
              <strong>International Conference on Artificial Neural Networks (ICANN), 2024</strong>
              <p> 
                A multi-order factor fusion framework is designed to fuse text, audio, and video information for depression diagnosis.
              </p>
              
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mmff'>
                  <img src="images/research/mft.png" style="width:140%;">
                </div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle">
              <papertitle class="blue-color">MFT: Multi-scale Fusion Transformer for Infrared and Visible Image Fusion</papertitle>
              <br>
              <a class="author-color">Chen-Ming Zhang</a>, 
              <a class="author-color"><strong>Chengbo Yuan</strong></a>,
              <a href="https://scholar.google.com/citations?user=zb1oVGIAAAAJ&hl=zh-CN&oi=ao" class="author-color">Yong Luo</a>,
              <a href="hhttps://scholar.google.com/citations?user=-PIr0aMAAAAJ&hl=zh-CN&oi=sra" class="author-color">Xin Zhou</a>
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-44223-0_39">Paper</a>
              <br>
              <strong>International Conference on Artificial Neural Networks (ICANN), 2023</strong>
              <p> 
                A pyramid network architecture for infrared and visible image fusion.
              </p>
              
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="10"><tbody>
          <br>
              <heading>Project</heading>
          <br>
          <tr>
            <td style="padding-left:10px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/project/MindQF.png", width="150"></td>
            <td width="90%" valign="center">
              <papertitle class="blue-color">MindQuantFinance: High-Performance Quantitative Asset Pricing Library</papertitle>
              <br>
              <strong> A library for asset pricing (Q-quant) based on MindSpore </strong>
              <br>
              <a href="https://gitee.com/luweizheng/mind-quant-finance">gitee</a>
              <br>
              This is an AI+Finance project I participated in, where we built a high-performance library for derivative pricing based on <a href="https://www.mindspore.cn/en">MindSpore</a>. 
              It offers features such as Black-Scholes calculations, Monte Carlo simulations, and solutions to Backward SDE.
            </td>
          </tr> 
          <tr>
            <td style="padding-left:10px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/project/qfr_fold.jpg", width="150"></td>
            <td width="90%" valign="center">
              <papertitle class="blue-color">QFR: Tabular Prediction for Quantative Finance Research</papertitle>
              <br>
              <strong> A tabular prediction library designed for financial stock prediction. </strong>
              <br>
              <a href="https://github.com/michaelyuancb/quant_finance_research">github</a>
              <br>
              This is one of the outcomes of an interesting experience, during which I worked with <a href="https://people.iiis.tsinghua.edu.cn/~jianli/">Prof. Jian Li</a> on stock prediction problems.
              It provide rich tools for factors extraction and results prediction.
            </td>
          </tr>  
          <tr>
            <td style="padding-left:10px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/project/tao_class.png", width="150"></td>
            <td width="90%" valign="center">
              <papertitle class="blue-color">TaoClass: an AI-Driven Course Forum</papertitle>
              <br>
              <strong> A course forum powered by Large Language Models (LLMs) and Vector Databases (VD). </strong>
              <br>
              <a href="https://gitee.com/yuanchengbo1/whu_taoclass">gitee</a>
              <br>
              This is a project I led, and it also serves as the final assignment for the Software Engineering course at Wuhan University. 
              The forum utilizes AI technology to offer natural language course selection and intelligent back-end information management.
            </td>
          </tr>   
          
        </tbody></table>
        
<!-- 
        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <br>
            <heading>Service and Teaching</heading>
            <br><br>
          <td style="padding:0px;width:100%;vertical-align:middle">
            <p>
              <li>Reviewer: The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR 2024)</li>
            </p>
            <p>
              <li>Reviewer: 2024 IEEE Robotics and Automation Letters (RAL 2024)</li>
            </p>
          </td>
        </tr>
        </tbody></table> -->

        <!-- <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <br>
              <heading>Recent Talks</heading>
            <br><br>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <p>
                <li>[August 20] Student Representative Speech at the Annual School Assembly of EECS
                [<a href="pdf\Speech_2023_Assembly.pdf">Manuscript</a>]
                </li>
             </p>
              <p>
                 <li>[May 27] "RLAfford: End-to-End Affordance Learning for Robotic Manipulation" at Turing Student Forum (<strong>Outstanding Presentation Award</strong>)
                 [<a href="https://www.bilibili.com/video/BV1pM4y1i77m/?spm_id_from=333.999.0.0&vd_source=29e736b58716f318ce554795bbbe6149">Video</a>]
                 </li>
              </p>
            </td>
          </tr>
        </tbody></table> -->

        </br>
        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <br>
                <heading>Selected Awards and Honors</heading>
                <br>
                <br>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <p>
                    <li><span style="background-color: lightyellow;">2023: Lei Jun Outstanding Scholarship of Wuhan University (<strong>Highest-level Scholarship</strong>, 10/60k+, ¬•100k RMB)</span></li>
                  </p>
                  <p>
                    <li>2023: Outstanding Camper of the Recruitment Camp of <a href="https://eng.pbcsf.tsinghua.edu.cn/">Tsinghua PBC School of Finance</a></li>
                  </p>
                  <p>
                    <li>2022: National Scholarship of Chinese Government (8000 RMB)</li>
                  </p>
                  <p>
                    <li><span style="background-color: lightyellow;">2019: National Olympiad in Informatics (NOI2019), <strong>Silver Award (Class D)</li>
                  </p>
                </td>
            </tr>
        </tbody></table>

        </br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <br>
            <heading>Gallery</heading>
            <br>
            <br>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <div class="gallery-container">
                <img src="images/gallery/season.jpg" class="gallery-item">
                <img src="images/gallery/f1.jpg" class="gallery-item">
                <img src="images/gallery/robot.jpg" class="gallery-item">
                <img src="images/gallery/Floruit.jpg" class="gallery-item">
                <img src="images/gallery/t.jpg" class="gallery-item">
                <img src="images/gallery/tang.jpg" class="gallery-item">
                <img src="images/gallery/interstellar.jpg" class="gallery-item">

                <img src="images/gallery/cat_c.jpg" class="gallery-item">
                <img src="images/gallery/xue.jpg" class="gallery-item">
                <img src="images/gallery/fox.jpg" class="gallery-item">
                <img src="images/gallery/L.jpg" class="gallery-item">
                <img src="images/gallery/sun.jpg" class="gallery-item">
                <img src="images/gallery/goat.jpg" class="gallery-item">
                <img src="images/gallery/h.jpg" class="gallery-item">
                
                <img src="images/gallery/ha.jpg" class="gallery-item">
                <img src="images/gallery/huachan.jpg" class="gallery-item">
                <img src="images/gallery/sun.jpg" class="gallery-item">
                <img src="images/gallery/nj.jpg" class="gallery-item">
                <img src="images/gallery/dog.jpg" class="gallery-item">
                <img src="images/gallery/avril.jpg" class="gallery-item">
                <img src="images/gallery/hp.jpg" class="gallery-item">
              </div>
            </td>
          </tr>
        </tbody></table>

        </br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    This template is a modification to <a href="https://gengyiran.github.io/">Yiran Geng</a>'s and <a href="https://jonbarron.info/">Jon Barron</a>'s website. 
                </p>
              </td>
            </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
</body>
</html>
